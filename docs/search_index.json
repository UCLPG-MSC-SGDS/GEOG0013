[["index.html", "GEOG0013: Geography in the Field 1 (GIF1) 2023/24 Welcome to Introduction to Statistics Structure Self-guided tutorials Questions &amp; Discusson Forum on Moodle", " GEOG0013: Geography in the Field 1 (GIF1) 2023/24 Welcome to Introduction to Statistics Welcome to GIF1’s Introduction to Statistics. So far you have been introduced to the core tenets and principles of geography taught through the different themes touching on physical sciences (e.g., coastal and rivers) and social sciences (e.g., ethnographic methods and urbanism). This session focuses on the themes of statistics (i.e., data analysis). Here, we will provide you with an introduction to the basic statistical methods for exploring &amp; representing secondary data from Barcelona, using a programming software tool known as R/RStudio, as well as using R/RStudio to introduce you to the basics of probability distributions and how they can be use for making predictions. Getting started with the learning process (Length: 13:50 minutes) Watch on YouTube [LINK] Structure The structure of this component of GIF1 is a series of two 3-hour workshop that will be delivered in-person. All students have been allocated to one of these groups (i.e., group A, B, C, D, E or G). The workshops will take place on the following dates: Week 7, Workshop 1 - Introduction to RStudio &amp; Descriptive Analysis: Wednesday 22nd November, 09:00am-12:00pm for groups A, B and C Friday 24th November, 02:00pm-05:00pm for groups D, E and G Group Location A IOE, Bedford Way (20) Room 425 (PC Lab 2), Map B IOE, Bedford Way (20) Room 429 (PC Lab 1), Map C Torrington Place (1-19) B07 Teal Room, Map D Bedford Way (26) Room 316 Public Cluster, Map E IOE, Bedford Way (20) Room 425 (PC Lab 2), Map G IOE, Bedford Way (20) Room W3.05, Map Week 8, Workshop 2 - Introduction to Distributions: Wednesday 29th November, 09:00am-12:00pm for groups A, B and C Friday 1st December, 02:00pm-05:00pm for groups D, E and G Group Location A IOE, Bedford Way (20) Room 425 (PC Lab 2), Map B IOE, Bedford Way (20) Room 429 (PC Lab 1), Map C Torrington Place (1-19) B07 Teal Room, Map D Bedford Way (26) Room 316 Public Cluster, Map E IOE, Bedford Way (20) Room 425 (PC Lab 2), Map G IOE, Bedford Way (20) Room W3.05, Map IMPORTANT NOTE 1: The locations for groups A, B, C, D and E are public clusters with PC workstations. Nevertheless, I highly recommend you bring your own laptop to the computer practicals for ease. For group G’s location at IOE, Bedford Way (20) Room W3.05 - you MUST bring your own laptop/tablet as this is NOT a public cluster with PC workstations. IMPORTANT NOTE 2: Unfortunately, the teaching assistant scheduled to deliver Friday’s RStudio &amp; Statistics workshop for Group F at the School of Pharmacy in Room B37 location WILL NOT be available to teach there. So, if you are in Group F, I have re-allocated you to another group. Please check your email to see your new group. Self-guided tutorials You have been given early access to the content. Prior to the workshops which will be supported by the teaching assistants (PGTAs), you are welcome to go through the self-guided tutorials presented on this website at your own pace. The teaching materials have been designed to be accessible to anyone who has never used RStudio in their life. It is also accessible to anyone who has never done statistics before as well. With that being said, we are introducing you to the absolute basics of R-programming and statistics. Hence, these tutorials are prescriptive with instructions which are further supported by guided videos. Questions &amp; Discusson Forum on Moodle On Moodle, you can use the Forum: Questions and Discussion platform to post general questions about the content on this webpage, or for posting problems on technical issues you have encountered during your self-guided study. Myself, or one of the PGTAs will respond accordingly with solutions. Students are welcome to engage and support each other in-person and/or through this forum during this self-study time. Please, do not send direct emails to Helen, Geography Office or me for resolving technical problems. We will only respond to emails on critical matters (e.g., personal problems, illness, etc.,). Kindly use the forum in Moodle - thank you! My contact information are: Anwar Musah | Lecturer in Social &amp; Geographic Data Science UCL Department of Geography University College London Room 115 North-West Wing, Gower Street, London, WC1E 6BT Email: a.musah@ucl.ac.uk "],["introduction.html", "Introduction Learning outcomes", " Introduction The goal in the next two weeks are threefold - the first part is to get you started with using RStudio and being familiar with its environment. The second part of the session aims to introduce you to the basic programming etiquette for basic data management. The third part is focused on building your confidence for using RStudio for data analysis such as descriptive analysis. The last part is to get you to understand what distributions are and using them for basic probability predictions. At the end of this workshop, you should be able to perform some basic data managing and analysis task in RStudio. The skills learned here will enable you complete the Formative Assignment and prepare you for what’s next in GIF2. Learning outcomes Across the three parts, the learning outcomes for this workshop are partitioned are as follows: The first task includes getting you started with RStudio by installing the needed software(s) (i.e., RStudio and R (Base)) on to your personal laptop. The second task aims to get you to being familiar with its RStudio’s environment and panels. Here, we begin with you interacting with RStudio’s console to do simple arithmatic and creating objects. The third task we will begin a soft introduction on the basics of managing data in RStudio. This includes learning how to create various objects in RStudio such as vector and data frame objects which forms the basics of data structures. The crucial part of this session will be to teach how to set-up working directories, scripting and importing Barcelona datasets in RStudio. Next, we will learn how to handle the imported data for descriptive analysis using the following techniques: (a.) data types and visualisation; (b.) frequency distribution; and (c.) central tendency measures. Lastly, we will learn the following techniques: (a.) generating theoretical probability distribution with particular focus on the Normal Distribution; and (b.) to simulate observations from Normal Distribution for making probability predictions based on the mean and standard deviation. These task will be supported with guidance videos to help with the self-guided learning. Alright, let do this [LINK]! "],["part-1-getting-started-with-rstudio.html", "Part 1: Getting started with RStudio What is RStudio (or R) Download and install RStudio directly on to your laptop (BEST OPTION) Using the UCL Desktop Cloud system to access RStudio Becoming familiar with the panels in RStudio Using R Console as a Calculator Creating basic objects and assigning values to it", " Part 1: Getting started with RStudio What is RStudio (or R) R, or RStudio is a statistical software programming package that allows the user to carry out different types of statistical analysis. It can also be used as a GIS software to perform various kinds of analysis on geographical data. In the same vein, you can use it for data managing and geo-processing (i.e., importing different types of data that non-spatial, or spatial formats for manipulation beforehand for analysis). There are two versions: The famous icon on the left is the version for R (Base), and the one on the right is the version for RStudio. Both software packages are the same. The only difference is that RStudio is attractive, intuitive, and more importantly, it is user-friendly than Base R. So, we will be using this version (i.e., RStudio) throughout this workshop. Let us talk about downloading RStudio. Getting started with RStudio (Length: 23:53 minutes) Watch on YouTube [LINK] Download and install RStudio directly on to your laptop (BEST OPTION) RStudio is an open source software, and today its the go-to software for many researchers - its highly recommended for anyone in the domains of data science, scientific research, and technical communication. It is easy to access, and easy to download and install. In order for RStudio to work you must first install R (Base). You can follow the steps and use the table below to download the correct version for your operating system (i.e., Windows or MAC). STEPS Download the file (i.e., .exe or .pkg) for R (Base) in accordance with your operating system from the links provided in the table below. Next, install it by clicking on the downloaded file (i.e., .exe or .pkg). Now, we can download the file (i.e., .exe or .dmg) for RStudio in accordance with your operating system from the links provided in the table below. You can install it by clicking on the downloaded file (i.e., .exe or .dmg). OS User type R (Base) RStudio Desktop Windows R-4.3.2-win.exe RStudio-2023.09.1-494.exe MAC R-4.3.2.pkg RStudio-2023.09.1-494.dmg Jump straight to this section if you managed to install RStudio successfully. Else, if you are having difficulties installing RStudio, and installation is a no go because you are using a iPad Pro, Chromebook or some other tablet - then here is an alternative solution. Use the UCL Desktop Cloud system to work remotely and gain access RStudio from your laptop/PC/tablet. Using the UCL Desktop Cloud system to access RStudio Remote access to RStudio (Length: 22:39 minutes) Watch on YouTube [LINK] To use RStudio (or any other software which UCL provides as service to students) remotely from your laptop/PC/tablet you can: Go to https://www.ucl.ac.uk/isd/services/computers/remote-access/desktopucl-anywhere Click on the blue button that say: “Log in to Desktop @ UCL Anywhere” You will be prompted to enter your UCL username (username[@]ucl.ac.uk) and password. Enter the correct credentials and you are granted access to a remote portal. If you see “Use Web browser” select this option. Or if a different option appears i.e., “Full” or “lite version”, then do select the “lite version” – so you can use the remote functions on the fly without having to install any Citrix Workspace Application. You should see a Desktop @ UCL Anywhere button – click on this button to finally be granted remote access At this stage, it like you are logging into a UCL Workstation in a cluster room, or library. Wait for it and you will be fully logged in. Click on the Start button (in the left-bottom corner) of the desktop, and go to the app section and scroll to the RStudio folder Click on the latest version UCL has RStudio 4.3.2 and open it once and not multiple times! Kindly wait until it opens. This is how you access RStudio remotely. Some general notes: Do open your internet browser within this remote window – not outside on you actually computer. Open the online worksheet and download the dataset there and not from you actual computer! When downloading, all downloads by default goes to the Download folder. UCL Desktops by default uses a Window OS. So, setting up the directory will be akin to how you do the set up normally on your windows PC. Go see the instructions in part two on how you set up work directory. However, because UCL has provided the N: Drive for us instead of C: Drive – you will hence need to set-up you working directory there which will look like: \"N:/GEOG0013/Workshop 1\" and not \"C:/Users/accountName/Desktop/GEOG0013/Workshop 1\" UCL Desktops provided students access to the N: Drive for storing data, work etc., – so make good use of this facility. Becoming familiar with the panels in RStudio Now that you have the set-up for RStudio (either installed or remotely). You should by now have opened RStudio. When opening RStudio for the first time, you are greeted with its interface. The window is split into three panels: 1.) R Console, 2.) Environments and 3.) Files, help &amp; Output. Panel 1: The Console lets the user type in R-codes to perform quick commands and basic calculations. Panel 2: The Environments lets the user see which datasets, spatial objects and other files are currently stored in RStudio’s memory Panel 3: Under the File tab, it lets the user access other folders stored in the computer to open datasets. Under the Help tab, it also allows the user to view the help menu for codes and commands. Finally, under the Plots tab, the user can perusal his/her generated plots (e.g., histogram, scatterplot, maps etc.). The above section is the Menu Bar. You can access other functions for saving, editing, and opening a new Script File for writing codes. Opening a new Script File will reveal a fourth panel above the Console. You can open a Script File by: Clicking on the File tab listed inside the Menu Bar. A scroll down bar will reveal itself. Here, you can scroll to the section that says New File. Under New File, click on R Script. This should open a new Script File titled “Untitled 1”. Important Notes: Throughout the course, and in all practical tutorials, you will be encouraged to use an R Script for collating and saving the codes you have written for carrying out spatial analysis. However, we will start writing codes in a script in part 2 of the tutorials. For now, let us start with the absolute basics, which is interacting with the R Console and using it as a basic calculator for typing simple code. Using R Console as a Calculator The R console window (i.e., Panel 1) is the place where RStudio is waiting for you to tell it what to do. It will show the code you have commanded RStudio to execute, and it will also show the results from that command. You can type the commands directly into the window for execution as well. Let us start by using the console window as a basic calculator for typing in addition (+), subtraction (-), multiplication (*), division (/), exponents (^) and performing other complex sums. Click inside the R Console window and type 19+8, and press enter key button ↵ to get your answer. Quickly perform the following maths by typing them inside the R Console window: # Perform addition 19+8 # Perform subtraction 20-89 # Perform multiplication 18*20 # Perform division 27/3 # To number to a power e.g., 2 raise to the power of 8 2^8 # Perform complex sums (5*(170-3.405)/91)+1002 Important Notes: The text that follows after the hash tag # in the above code chunk is a comment and actual code. It is there telling you what the code without hash tag # in front of it is doing. Aside from basic arithmetic operations, we can use some basic mathematical functions such as the exponential and logarithms: exp() is the exponential function log() is the logarithmic function Do not worry at all about these functions as you will use them later in GIF2 to come for transforming variables. Perform the following by typing them inside the R Console window: # use exp() to apply an exponential to a value exp(5) # use log() to transforrm a value on to a logarithm scale log(3) Creating basic objects and assigning values to it Now that we are familiar with using the console as a calculator. Let us build from this and learn one of the most important codes in RStudio which is called the Assignment Operator. This arrow symbol &lt;- is called the Assignment Operator. It is typed by pressing the less than symbol key &lt; followed by the hyphen symbol key -. It allows the user to assign values to an Object in R. Objects are defined as stored quantities in RStudio’s environment. These objects can be assigned anything from numeric values to character string values. For instance, say we want to create a numeric object called x and assign it with a value of 3. We do this by typing x &lt;- 3. When you enter the object x in the console and press enter ↵, it will return the numeric value 3. Another example, suppose we want to create a string object called y and assign it with some text \"Hello!\". We do this typing y &lt;- \"Hello!\". When you enter y in console, it will return the text value Hello. Let us create the objects a,b, c, and d and assign them with numeric values. Perform the following by typing them inside the R Console window: # Create an object called &#39;a&#39; and assign the value 17 to it a &lt;- 17 # Type the object &#39;a&#39; in console as a command to return value 17 a # Create an object called &#39;b&#39; and assign the value 10 to it b &lt;- 10 # Type the object &#39;b&#39; in console as a command to return value 10 b # Create an object called &#39;c&#39; and assign the value 9 to it c &lt;- 9 # Type the object &#39;c&#39; in console as a command to return value 9 c # Create an object called &#39;d&#39; and assign the value 8 to it d &lt;- 8 # Type the object &#39;d&#39; in console as a command to return value 8 d Notice how the objects a, b, c and d and its value are stored in RStudio’s environment panel. We can perform the following arithmetic operations with these object values: # type the following and return an answer (a + b + c + d)/5 # type the following and return an answer (5*(a-c)/d)^2 Let us create more objects but this time we will assign character string(s) to them. Please note that when typing a string of characters as data you will need to cover them with quotation marks \"...\". For example, say we want to create a string object called y and assign it with some text \"Hello!\". We do this by typing y &lt;- \"Hello!\". Try these examples of assigning the following character text to an object: # Create an object called &#39;e&#39; and assign the character string &quot;RStudio&quot; e &lt;- &quot;RStudio&quot; # Type the object &#39;e&#39; in the console as a command to return &quot;RStudio&quot; e # Create an object called &#39;f&#39;, assign character string &quot;Hello world&quot; f &lt;- &quot;Hello world&quot; # Type the object &#39;f&#39; in the console as a command to return &quot;Hello world&quot; f # Create an object called &#39;g&#39; and assign &quot;Blade Runner is amazing&quot; g &lt;- &quot;Blade Runner is amazing&quot; # Type the object &#39;g&#39; in the console to return the result g We are now familiar with using the console and assigning values (i.e., numeric and string values) to objects. The parts covered here are the initial steps and building blocks for coding and creating datasets in RStudio. Let us progress to the next section. We will learn the basics of managing data and some coding etiquette - this includes creating data frames, importing &amp; exporting spreadsheets, setting up work directories, column manipulations and merging two data frames. Learning these basic tasks are key for managing data in RStudio. Important Notes: We will be using R-scripts file for typing codes from this point onwards. "],["part-2-basics-of-managing-data-in-rstudio.html", "Part 2: Basics of Managing Data in RStudio How do we enter data into RStudio? How do we create a variable based on other existing variables in our data frame? How to set the working directory with setwd() function? Importing data using read.csv() Joining two datasets using the merge() function Saving your dataset using the write.csv() function", " Part 2: Basics of Managing Data in RStudio Data entry &amp; column generation (Length: 19:23 minutes) Watch on YouTube [LINK] How do we enter data into RStudio? As you have already seen, RStudio is an object-oriented software package and so entering data is slightly different for the usual way of inputting information into a spreadsheet (e.g., Microsoft Excel). Here, you will need to enter the information as a Vector object before combining them into a Data Frame object. Consider this crude example of data containing the additional health information for 4 people. It contains the variable (or column) names ‘id’, ‘name’, ‘height’, ‘weight’ and ‘gender’ id name height weight gender 1 Kofi 1.65 64.2 M 2 Harry 1.77 80.3 M 3 Huijun 1.70 58.7 F 4 Fatima 1.68 75.0 F Now, when entering data to RStudio it is not like Microsoft Excel where we enter data into the cells of a spreadsheet. In RStudio, data is entered as a sequence of elements and listed inside an object called a vector. For instance, if we have three age values of 12, 57 and 26 years, and we want to enter this in RStudio, we need to use the combine function c() and combine these three elements into a vector object. Hence, the code will be c(12, 57, 26). We can assign this data by typing this code as age &lt;- c(12, 57, 26). Any time you type ‘age’ into RStudio console it will hence return these three values unless you chose to overwrite it with different information. Let us look at this more closely with the 'id' variable in the above data. Each person has an ID number from 1 to 4. We are going to list the numbers 1, 2, 3 and 4 as a sequence of elements into a vector using the combine function c() and then assign it to as a vector object calling it 'id'. # Create &#39;id&#39; vector object id &lt;- c(1, 2, 3, 4) # Type the vector object &#39;id&#39; in console to see output id Now, let us enter the information the same way for the remaining columns for ‘name’, ‘height’, ‘weight’ and ‘gender’ like we did for ‘id’: # Create &#39;name&#39; vector object name &lt;- c(&quot;Kofi&quot;, &quot;Harry&quot;, &quot;Huijun&quot;, &quot;Fatima&quot;) # Create &#39;height&#39; (in meters) vector object height &lt;- c(1.65, 1.77, 1.70, 1.68) # Create &#39;weight&#39; (in kg) vector object weight &lt;- c(64.2, 80.3, 58.7, 75.0) # Create &#39;gender&#39; vector object gender &lt;- c(&quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;) Now, that we have the vector objects ready. Let us bring them together to create a proper dataset. This new object is called a Data frame. We need to list the vectors inside the data.frame() function. # Create a dataset (data frame) dataset &lt;- data.frame(id, name, height, weight, gender) # Type the data frame object &#39;dataset&#39; in console to see output dataset # You can also see dataset in a data viewer, type View() to data: View(dataset) Important Notes: The column ‘id’ is a numeric variable with integers. The second column ‘name’ is a text variable with strings. The third &amp; fourth columns ‘height’ and ‘weight’ are examples of numeric variables with real numbers with continuous measures. The variable ‘gender’ is a text variable with strings – however, this type of variable is classed as a categorical variable as individuals were categorised as either ‘M’ and ‘F’. How do we create a variable based on other existing variables in our data frame? To access a variable by its name within a data frame, you will need to first type the name of the data frame followed by a $ (dollar sign), and then typing the variable’s name of interest. For instance, suppose you just want to see the height values in the Console viewer - you just type: # to access height - you need to type &#39;dataset$height&#39; dataset$height We can use other columns or variables within our data frame to create another variable. This technique is essentially important when cleaning and managing data. From this dataset, it is possible to derive the body mass index bmi from height and weight using the formula: \\(BMI = weight/height^2\\) To generate bmi into our data frame, we would need to access the height (m) and weight (kg) columns using the $ from the data frame its stored to, and apply the above formula as a code to generate the new bmi column: # Create &#39;bmi&#39; in the data frame i.e.,&#39;dataset&#39; and calculate &#39;bmi&#39; # using the $weight and $height dataset$bmi &lt;- dataset$weight/((dataset$height)^2) # View the data frame ‘dataset’ and you will see the new bmi variable inside View(dataset) You can overwrite the height (m) column to change its units into centimeters by multiplying it to 100; equally, the weight (kg) column can be overwritten and converted from units of kilograms to grams by multiplying it to 1000. # using $height and *100 dataset$height &lt;- dataset$height*100 # using $weight and *100 dataset$weight &lt;- dataset$weight*1000 # use View() the data frame ‘dataset’ and you will see the updated variables View(dataset) How to set the working directory with setwd() function? Set-up work directory and importing data (Length: 25:43 minutes) Watch on YouTube [LINK] IMPORTANT ACTION: Please download the data set for this practical by clicking here. Now, in your computer, please create a new folder on your desktop page and rename the folder to “GEOG0113”, and create another folder within “GEOG0013” and rename it as “Workshop 1”. Make sure to unzip and transfer ALL the downloaded data directly to the Workshop 1 folder. Now, we are getting very serious here! This part of the practicals are probably the most important section of this tutorial. It’s usually the “make” or “break” phase (i.e., you ending up loving RStudio OR you hating it and not ever wanting to pick up R again). We are going to learn how to set-up a working directory. This basically refers to us connecting the RStudio to the folder containing our dataset. It allows the user to tell RStudio to open data from a folder once it knows the path location. The path location specifies the whereabouts of the data file(s) stored within a computer. Setting your directory in RStudio beforehand makes life incredibly easier in terms of finding, importing, exporting and saving data in and out of RStudio. To illustrate what a path location is – suppose on my desktop (mac/widows) there is a folder called “GEOG0013”, and within that folder, exists another folder called “Workshop 1”. Finally, suppose a comma separated value (.csv) data file called “Barcelona_rents_2015.csv” is store in this last folder i.e., Workshop 1. If via RStudio you want to open this CSV data file located in within the “Workshop 1” folder. You will need to first set the path to “Workshop 1” in RStudio using the setwd() function. Therefore, the path location to this folder on a Windows machine would be written as follows, \"C:/Users/accountName/Desktop/GEOG0013/Workshop 1\". You can access this piece of information simply by: Open the GEOG0013 folder to reveal the Workshop 1 folder. Open the Workshop 1 folder in the data files are stored. Now, click on the bar at the top which shows GEOG0013 &gt; Workshop 1. This should highlight and show \"C:\\Users\\accountName\\Desktop\\GEOG0013\\Workshop 1\" (see image below): Now, copy \"C:\\Users\\accountName\\Desktop\\GEOG0013\\Workshop 1\" and paste the path name into the setwd() function in your R script. Lastly, change all the back slashes \\ in the path name to forward slashes / and run the code. It should look like this: setwd(\"C:/Users/accountName/Desktop/GEOG0013/Workshop 1\"). For Windows, the setwd() is as follows: # set work directory in windows setwd(&quot;C:/Users/accountName/Desktop/GEOG0013/Workshop 1&quot;) For MAC users, its marginally different. The path location would be written as follows, \"/Users/accountName/Desktop/GEOG0013/Workshop 1\". You can access this piece of information simply by: Right-clicking on the folder “Workshop 1” (not file) in which the files are stored. Hold the “Option” ⌥ key down Click Copy \"filename\" as Pathname Paste the copied path name into the function setwd() and run the code For Mac, the setwd() is as follows: # set work directory in macs setwd(&quot;/Users/accountName/Desktop/GEOG0013/Workshop 1&quot;) This should set the working directory. Alternatively, if the setwd() is problematic - you can do this manually. See short video below. Setting Work Directory Manually if Using the setwd() function is problematic (Length: 3:58 minutes) Watch on YouTube [LINK] IMPORTANT ACTION: Again, please make sure to have downloaded the data set for this practical by clicking here. Also, in your computer, please make sure you have already created a new folder on your desktop page and renamed the folder to “GEOG0113”, and have also created another folder within “GEOG0013” and have renamed that to “Workshop 1”. Make sure to unzip and transfer ALL the downloaded data directly to the Workshop 1 folder. Now, let us learn how to import a CSV data into RStudio. Importing data using read.csv() As you will be working mostly with comma separated value formatted data (i.e., csv) we will therefore learn how to import and export in RStudio. There are two files that we are going to import into RStudio from Week 1’s folder: Barcelona_cars_2015.csv which contains an indicator for proportion of car ownership in 73 Spanish neighbourhoods in Barcelona in 2015. Barcelona_rents_2015.csv which contains an indicator for average monthly rent (in euros) spent in 73 Spanish neighbourhoods in Barcelona in 2015. To import a csv into RStudio, we use the read.csv() function. To demonstrate this, let us import the data for average monthly rents into an data frame object and name it as Rent_data # Import data using read.csv() function Rent_data &lt;- read.csv(file=&quot;Barcelona_rents_2015.csv&quot;, header = TRUE, sep = &quot;,&quot;) Just in case…suppose if we did NOT set the working directory earlier. We would have to go through the hassle of typing the path location in the read.csv(). For windows: Rent_data &lt;- read.csv(file=&quot;C:/Users/accountName/Desktop/GEOG0013/Workshop 1/Barcelona_rent_2015.csv&quot;, header = TRUE, sep = &quot;,&quot;) For Mac: Rent_data &lt;- read.csv(file=&quot;/Users/accountName/Desktop/GEOG0013/Workshop 1/Barcelona_rent_2015.csv&quot;, header = TRUE, sep = &quot;,&quot;) I do not recommend doing it this way. Just set the work directory with setwd() to make life easier for yourself. Important Notes: The arguments used in read.csv() function – 1.) ‘file =’ is a mandatory option where you quote the name of the file to be imported; 2.) ‘header = TRUE’ option is set to TRUE which is telling RStudio that the file that is about to be imported has column names on the first row so it should not treat as observations; and 3.) ‘sep = \",\"’ we are telling RStudio that the format of the dataset is comma separated. We have imported the Barcelona_rents_2015.csv data. Now, let us import the second data for Barcelona_cars_2015.csv using the read.csv() function and call it Cars_data. The code would look something as follows: # Import data using read.csv() function Cars_data &lt;- read.csv(file=&quot;Barcelona_cars_2015.csv&quot;, header = TRUE, sep = &quot;,&quot;) # Show viewer the data sets View(Rent_data) View(Cars_data) Joining two datasets using the merge() function Merging and saving datasets (Length: 23:15 minutes) Watch on YouTube [LINK] In your journey with data sets, you will certainly find yourself merging two or more data frames together, especially bringing together a spatial object with a non-spatial object. We cannot stress the importance of merging objects in the correct order so that the spatial attributes are preserved. It is possible to merge the two data frames uniquely using a common key variable like neighbourhoods that is present in both data sets. This task can be done using the merge function merge(). Consequently, we want the format of the merge code to look something akin to this syntax merge(target_object, selected_object, by=”Key_variable”). Merging data frames is indeed a very important technique to know especially if you need to bring together event information with no spatial dimension with actual spatial data. Alright, let’s merge the cars ownership information on the home rental records using the Key_variable column, and generate a bigger data frame that contains both the rental and car ownership information: # Using the merge() function Barcelona_data &lt;- merge(Rent_data, Cars_data, by.x = &quot;neighbourhood&quot;, by.y = &quot;neighbourhood&quot;, all.x = TRUE) # View the datasets View(Barcelona_data) Important Notes: The arguments used in merge.csv(): Rent_data is the target data frame we want something to be merged on to. Cars_data is the selected data frame we are using to merge with the Rent_data. by.x = “neighbourhood” option we are specifying the name of the join column from the target data frame i.e., Rent_data. by.y = “neighbourhood” option we are specifying the name of the join column from the selected data frame i.e., Cars_data all.x=TRUE option we are telling RStudio to retain all rows that are originally from the target data after merging regardless of whether or not they are present in the selected data frame. So even if a row from the selected data does not find a unique link with any of the rows in target data to match too - it will still preserve the target data frame by not discarding unlinked rows. But it will discard the unmatched rows from the selected data frame. Saving your dataset using the write.csv() function Let us save a version of this as a .csv file as a saved product named “Barcelona_Data.csv”. This can be done by using the write.csv() function. It will export the data frame object into a .csv format. # Export ‘Barcelona_Data’ object as .csv into &#39;Week 1&#39; folder write.csv(Barcelona_Data, file = &quot;Barcelona_Data.csv&quot;, row.names = FALSE) Important Notes: The arguments used in merge.csv(): Barcelona_Data is an object we are exporting. It is compulsory to specify the object data frame we want to export file = is a mandatory argument. We must give a name to the file we wish to export it as with .csv extension. row.names = this is an annoying argument! It will automatically index the dataset with unique row numbers by default if we do not specify FALSE! Since the data has its own unique identifiers (i.e., neighbourhoods) we specify ‘FALSE’ to not perform this action of indexing Again, suppose if you did NOT set the work directory to your folder, you will have to type the whole path location to where you want the data to be exported which could be a hassle: For Windows: write.csv(Full_data, file = &quot;C:/Users/accountName/Desktop/GEOG0013/Workshop 1/Barcelona_data.csv&quot;, row.names = FALSE) For Mac: write.csv(Full_data, file = &quot;/Users/accountName/Desktop/GEOG0013/Workshop 1/Barcelona_data.csv&quot;, row.names = FALSE) Again, I do not recommend doing it this way. Just set the work directory with setwd() to make life easier for yourself and to avoid R calling you out for errors. Now that we have learned a lot of the basic things in RStudio – the stuff shown in Part 2 will be used quite a lot moving forward in journey of studies. Now, let us progress to the meat and potatoes in *Part 3** where will start using RStudio for statistical analysis. Here, we will conduct some descriptive profile of the air quality using pollution data from Barcelona. We’re in the final stretch now. "],["part-3-exploratory-analysis-in-rstudio.html", "Part 3: Exploratory analysis in RStudio What is statistics and data in general? Analysing air pollution data in Barcelona Frequency distributions Descriptive and central tendency measures", " Part 3: Exploratory analysis in RStudio What is statistics and data in general? Definition 1: Statistics is a branch in the mathematical sciences that pertains to the collection, analysis, interpretation, and graphical presentation of data. The best thing about statistics is that it’s a highly applied branch of science which is applicable to many areas such as social science, politics, health (e.g., epidemiology), business &amp; finance, environmental sciences and geography. Statistics is broadly split into two main areas: Descriptive statistics, which focuses on describing the visible characteristics about a dataset Inferential statistics is more research-based, which focuses on making predictions (rather than stating facts) and testing hypothesis about a phenomenon. Definition 2: A variable is any characteristics, numbered value, or quantity that can be measured or counted. A variable can also be referred to a data Item. A variable can be broadly classified as discrete, continuous or categorical variable. We have provided two videos: the first which broadly explains why statistics as a subject is important; and the second explains in much details what statistics is as a subject, and what are the various data types. [Theory] Why is statistics important? (Length: 13:21 minutes) Watch on YouTube [LINK] [Theory &amp; Code] What is statistics, and the types of variables? (Length: 31:17 minutes) Watch on YouTube [LINK] Analysing air pollution data in Barcelona We will focus on descriptive statistics as an introduction introducing everyone to the absolute basics. Descriptive statistics is all about knowing the data types and finding the distribution, central tendency and variability in such data set. These four key words may sound intimidating – but trust me – it is very easy! Let us learn how to perform this in RStudio using the air pollution data for Barcelona. Let us import for following dataset Barcelona_Air_Pollution_data.csv into RStudio, and call this object air_ quality_data. Remember - always make sure that your work directory is linked to your folder containing your data. For Windows: setwd(&quot;C:/Users/accountName/Desktop/GEOG0013/Workshop 1/&quot;) For Macs: setwd(&quot;/Users/accountName/Desktop/GEOG0013/Workshop 1/&quot;) Now, import you the data set as follows: air_quality_data &lt;- read.csv(&quot;Barcelona_Air_Pollution_data.csv&quot;) You use the command View() see the full data viewer, or head() to see the first five rows of the dataset. # see imported dataset View(air_quality_data) head(air_quality_data) You will notice that the data contains six variables with the following information: Variable name Variable Type Information Location String/Text only Name of location Eixample, Barcelona ReadingDate Date Data collection date for air quality measures NO2_est Continuous Measurements for Nitrogen dioxide (NO\\(_2\\)) (ppb) NO2_category Categorical Health impacts (negligible/low/moderate/high) PM10_est Continuous Measurements for Particulate matter (PM10) PM10_category Categorical Health impacts (negligible/low/moderate) Important Notes: The NO2_est, for example, contains measurable items i.e., 718 observations for concentrations of ambient NO\\(_2\\) in Eixample area of Barcelona, and hence its a continuous variable. These estimates have been categorised in accordance with their health dangers i.e., negligible (\\(&lt;\\) 10 ppb); low (11-50 ppb); moderate (51-100 ppb) and high (&gt;101 pbb). The categories are contained in the variable NO2_category. Let us begin to analyse NO2_est and NO2_category with frequency distributions Frequency distributions We use frequency distribution to analyse a set continuous data. In data handling in this context, there are two outputs generated: Frequency, which tells us how often a particular result was obtained. From this we can calculate a percentage value which is referred to as Relative Frequency. Cumulative Frequency, this is a cumulative sum of the frequencies, which indicates how often a result was obtained that is less than a stated value in our collection of data. Again, from this we can also calculate a cumulative percentage value which is referred to as Cumulative Relative Frequency. Suppose, we want to assess the 718 observations for air pollutant Nitrogen Dioxide (NO\\(_2\\)). Let’s list the observations for Nitrogen Dioxide (NO\\(_2\\)) in Barcelona: air_quality_data$NO2_est ## [1] 61 59 29 75 23 49 43 35 83 75 71 56 54 44 41 54 62 56 ## [19] 26 42 71 86 85 52 56 45 68 86 69 71 4 82 43 51 114 43 ## [37] 18 58 24 53 98 53 100 53 49 49 46 82 77 67 76 52 61 80 ## [55] 77 70 56 49 42 73 64 33 71 72 13 37 26 46 84 72 65 76 ## [73] 90 46 61 81 64 62 58 78 83 37 130 43 20 40 102 68 48 74 ## [91] 52 43 80 71 42 84 44 121 41 66 44 50 38 75 41 45 48 63 ## [109] 53 63 63 46 34 87 75 74 36 69 46 15 80 75 83 95 5 65 ## [127] 21 84 68 32 45 73 53 31 85 91 73 46 25 75 70 84 68 65 ## [145] 58 113 62 60 55 69 82 100 105 47 60 103 53 34 39 22 21 71 ## [163] 85 56 73 61 24 44 47 49 100 64 91 79 42 32 33 84 43 61 ## [181] 63 49 80 46 58 45 37 66 60 75 35 75 48 43 57 67 54 38 ## [199] 22 51 69 51 64 32 20 52 42 65 69 47 40 34 34 51 57 43 ## [217] 52 86 53 43 54 75 56 62 41 84 41 22 83 76 51 31 50 65 ## [235] 76 77 61 50 75 49 47 65 78 39 51 49 75 45 50 69 86 75 ## [253] 89 68 84 90 90 56 106 63 90 57 38 86 22 39 19 61 44 63 ## [271] 52 42 46 56 40 69 62 42 54 17 49 84 34 89 65 53 78 67 ## [289] 55 61 39 82 58 15 63 76 55 80 56 79 72 58 74 27 93 40 ## [307] 40 58 79 81 123 84 37 87 38 49 91 50 59 69 57 68 53 38 ## [325] 51 78 71 72 55 70 56 63 85 78 64 23 84 43 46 33 59 58 ## [343] 47 64 68 89 76 86 116 52 34 63 40 41 72 87 37 62 38 68 ## [361] 88 39 59 77 75 112 44 40 90 37 66 61 65 50 79 79 36 36 ## [379] 12 86 40 62 63 71 53 30 44 76 41 62 77 80 62 86 37 48 ## [397] 80 55 56 49 84 48 49 84 60 76 28 77 41 57 55 51 54 54 ## [415] 11 43 38 86 30 23 78 29 80 16 48 90 44 42 50 54 45 42 ## [433] 70 49 67 73 60 42 99 97 77 46 52 24 75 30 70 81 53 17 ## [451] 63 59 44 41 67 56 58 111 43 47 49 58 36 72 36 103 63 77 ## [469] 65 42 42 79 41 24 59 50 46 55 77 91 54 70 73 53 80 53 ## [487] 72 67 95 57 87 39 73 56 34 56 75 74 72 42 119 55 43 69 ## [505] 55 52 77 63 108 43 61 47 117 80 61 67 78 49 42 35 58 54 ## [523] 36 84 56 72 70 40 59 71 56 49 66 52 48 60 54 73 66 67 ## [541] 70 93 65 60 13 83 49 42 62 63 50 46 54 94 73 54 74 54 ## [559] 10 71 41 17 75 55 54 54 83 47 49 90 76 89 83 43 76 67 ## [577] 75 88 59 60 34 36 63 42 59 71 73 73 40 74 53 56 99 46 ## [595] 46 64 37 20 84 86 47 57 54 56 78 73 65 72 37 57 38 46 ## [613] 43 57 98 32 98 53 86 59 63 42 60 60 51 58 59 83 67 42 ## [631] 74 62 84 67 49 76 2 44 51 69 69 87 49 18 73 66 81 78 ## [649] 82 69 50 36 71 60 7 50 49 60 55 25 58 76 69 61 88 45 ## [667] 59 59 91 61 81 81 83 71 108 99 46 69 38 54 59 16 75 81 ## [685] 35 63 65 71 57 53 58 37 39 43 64 76 56 72 87 74 74 77 ## [703] 46 87 53 60 47 10 61 35 36 60 71 45 47 79 37 123 In a list format it is quite difficult to make head or tail on what observations appear frequently and its distribution. To summarise this - it will be helpful to classify the information into Classes and then obtain the Frequency and Cumulative Frequency in a table. We call this table a Frequency Table. The minimum value for NO\\(_2\\) is 2 and the maximum is 130. We can group the 718 observations into 13 classes using an interval of 10s e.g., 1-10, 11-20, 21-30, 31-40, 41-50, 51-60, 61-70, 71-80, 81-90, 91-100, 101-110, 111-120 and 121-130 Important Notes: The way and manner you specify the classes and interval are up to you really. Here, 10 is being used for convenience. The interval width is 10, we can generate sequence of number from 0 to 130, inclusively, to create the classes which in turn be used to group the 718 observations into 13 classes using the seq() and cut(). For example: # using starting value as 0 # using highest value as 130 # using interval as 10 # specify in this order the lower, highest, interval value in seq() function classes &lt;- seq(0, 130, 10) classes ## [1] 0 10 20 30 40 50 60 70 80 90 100 110 120 130 The sequence of values are stored in the object called classes. Now, let us apply the cut() function to group the NO\\(_2\\) data accordingly. We can do this by generating a new variable called Groups. # tell the cut() function to group NO2_est using the classes object air_quality_data$Groups &lt;- cut(air_quality_data$NO2_est, breaks=classes) air_quality_data$Groups ## [1] (60,70] (50,60] (20,30] (70,80] (20,30] (40,50] (40,50] ## [8] (30,40] (80,90] (70,80] (70,80] (50,60] (50,60] (40,50] ## [15] (40,50] (50,60] (60,70] (50,60] (20,30] (40,50] (70,80] ## [22] (80,90] (80,90] (50,60] (50,60] (40,50] (60,70] (80,90] ## [29] (60,70] (70,80] (0,10] (80,90] (40,50] (50,60] (110,120] ## [36] (40,50] (10,20] (50,60] (20,30] (50,60] (90,100] (50,60] ## [43] (90,100] (50,60] (40,50] (40,50] (40,50] (80,90] (70,80] ## [50] (60,70] (70,80] (50,60] (60,70] (70,80] (70,80] (60,70] ## [57] (50,60] (40,50] (40,50] (70,80] (60,70] (30,40] (70,80] ## [64] (70,80] (10,20] (30,40] (20,30] (40,50] (80,90] (70,80] ## [71] (60,70] (70,80] (80,90] (40,50] (60,70] (80,90] (60,70] ## [78] (60,70] (50,60] (70,80] (80,90] (30,40] (120,130] (40,50] ## [85] (10,20] (30,40] (100,110] (60,70] (40,50] (70,80] (50,60] ## [92] (40,50] (70,80] (70,80] (40,50] (80,90] (40,50] (120,130] ## [99] (40,50] (60,70] (40,50] (40,50] (30,40] (70,80] (40,50] ## [106] (40,50] (40,50] (60,70] (50,60] (60,70] (60,70] (40,50] ## [113] (30,40] (80,90] (70,80] (70,80] (30,40] (60,70] (40,50] ## [120] (10,20] (70,80] (70,80] (80,90] (90,100] (0,10] (60,70] ## [127] (20,30] (80,90] (60,70] (30,40] (40,50] (70,80] (50,60] ## [134] (30,40] (80,90] (90,100] (70,80] (40,50] (20,30] (70,80] ## [141] (60,70] (80,90] (60,70] (60,70] (50,60] (110,120] (60,70] ## [148] (50,60] (50,60] (60,70] (80,90] (90,100] (100,110] (40,50] ## [155] (50,60] (100,110] (50,60] (30,40] (30,40] (20,30] (20,30] ## [162] (70,80] (80,90] (50,60] (70,80] (60,70] (20,30] (40,50] ## [169] (40,50] (40,50] (90,100] (60,70] (90,100] (70,80] (40,50] ## [176] (30,40] (30,40] (80,90] (40,50] (60,70] (60,70] (40,50] ## [183] (70,80] (40,50] (50,60] (40,50] (30,40] (60,70] (50,60] ## [190] (70,80] (30,40] (70,80] (40,50] (40,50] (50,60] (60,70] ## [197] (50,60] (30,40] (20,30] (50,60] (60,70] (50,60] (60,70] ## [204] (30,40] (10,20] (50,60] (40,50] (60,70] (60,70] (40,50] ## [211] (30,40] (30,40] (30,40] (50,60] (50,60] (40,50] (50,60] ## [218] (80,90] (50,60] (40,50] (50,60] (70,80] (50,60] (60,70] ## [225] (40,50] (80,90] (40,50] (20,30] (80,90] (70,80] (50,60] ## [232] (30,40] (40,50] (60,70] (70,80] (70,80] (60,70] (40,50] ## [239] (70,80] (40,50] (40,50] (60,70] (70,80] (30,40] (50,60] ## [246] (40,50] (70,80] (40,50] (40,50] (60,70] (80,90] (70,80] ## [253] (80,90] (60,70] (80,90] (80,90] (80,90] (50,60] (100,110] ## [260] (60,70] (80,90] (50,60] (30,40] (80,90] (20,30] (30,40] ## [267] (10,20] (60,70] (40,50] (60,70] (50,60] (40,50] (40,50] ## [274] (50,60] (30,40] (60,70] (60,70] (40,50] (50,60] (10,20] ## [281] (40,50] (80,90] (30,40] (80,90] (60,70] (50,60] (70,80] ## [288] (60,70] (50,60] (60,70] (30,40] (80,90] (50,60] (10,20] ## [295] (60,70] (70,80] (50,60] (70,80] (50,60] (70,80] (70,80] ## [302] (50,60] (70,80] (20,30] (90,100] (30,40] (30,40] (50,60] ## [309] (70,80] (80,90] (120,130] (80,90] (30,40] (80,90] (30,40] ## [316] (40,50] (90,100] (40,50] (50,60] (60,70] (50,60] (60,70] ## [323] (50,60] (30,40] (50,60] (70,80] (70,80] (70,80] (50,60] ## [330] (60,70] (50,60] (60,70] (80,90] (70,80] (60,70] (20,30] ## [337] (80,90] (40,50] (40,50] (30,40] (50,60] (50,60] (40,50] ## [344] (60,70] (60,70] (80,90] (70,80] (80,90] (110,120] (50,60] ## [351] (30,40] (60,70] (30,40] (40,50] (70,80] (80,90] (30,40] ## [358] (60,70] (30,40] (60,70] (80,90] (30,40] (50,60] (70,80] ## [365] (70,80] (110,120] (40,50] (30,40] (80,90] (30,40] (60,70] ## [372] (60,70] (60,70] (40,50] (70,80] (70,80] (30,40] (30,40] ## [379] (10,20] (80,90] (30,40] (60,70] (60,70] (70,80] (50,60] ## [386] (20,30] (40,50] (70,80] (40,50] (60,70] (70,80] (70,80] ## [393] (60,70] (80,90] (30,40] (40,50] (70,80] (50,60] (50,60] ## [400] (40,50] (80,90] (40,50] (40,50] (80,90] (50,60] (70,80] ## [407] (20,30] (70,80] (40,50] (50,60] (50,60] (50,60] (50,60] ## [414] (50,60] (10,20] (40,50] (30,40] (80,90] (20,30] (20,30] ## [421] (70,80] (20,30] (70,80] (10,20] (40,50] (80,90] (40,50] ## [428] (40,50] (40,50] (50,60] (40,50] (40,50] (60,70] (40,50] ## [435] (60,70] (70,80] (50,60] (40,50] (90,100] (90,100] (70,80] ## [442] (40,50] (50,60] (20,30] (70,80] (20,30] (60,70] (80,90] ## [449] (50,60] (10,20] (60,70] (50,60] (40,50] (40,50] (60,70] ## [456] (50,60] (50,60] (110,120] (40,50] (40,50] (40,50] (50,60] ## [463] (30,40] (70,80] (30,40] (100,110] (60,70] (70,80] (60,70] ## [470] (40,50] (40,50] (70,80] (40,50] (20,30] (50,60] (40,50] ## [477] (40,50] (50,60] (70,80] (90,100] (50,60] (60,70] (70,80] ## [484] (50,60] (70,80] (50,60] (70,80] (60,70] (90,100] (50,60] ## [491] (80,90] (30,40] (70,80] (50,60] (30,40] (50,60] (70,80] ## [498] (70,80] (70,80] (40,50] (110,120] (50,60] (40,50] (60,70] ## [505] (50,60] (50,60] (70,80] (60,70] (100,110] (40,50] (60,70] ## [512] (40,50] (110,120] (70,80] (60,70] (60,70] (70,80] (40,50] ## [519] (40,50] (30,40] (50,60] (50,60] (30,40] (80,90] (50,60] ## [526] (70,80] (60,70] (30,40] (50,60] (70,80] (50,60] (40,50] ## [533] (60,70] (50,60] (40,50] (50,60] (50,60] (70,80] (60,70] ## [540] (60,70] (60,70] (90,100] (60,70] (50,60] (10,20] (80,90] ## [547] (40,50] (40,50] (60,70] (60,70] (40,50] (40,50] (50,60] ## [554] (90,100] (70,80] (50,60] (70,80] (50,60] (0,10] (70,80] ## [561] (40,50] (10,20] (70,80] (50,60] (50,60] (50,60] (80,90] ## [568] (40,50] (40,50] (80,90] (70,80] (80,90] (80,90] (40,50] ## [575] (70,80] (60,70] (70,80] (80,90] (50,60] (50,60] (30,40] ## [582] (30,40] (60,70] (40,50] (50,60] (70,80] (70,80] (70,80] ## [589] (30,40] (70,80] (50,60] (50,60] (90,100] (40,50] (40,50] ## [596] (60,70] (30,40] (10,20] (80,90] (80,90] (40,50] (50,60] ## [603] (50,60] (50,60] (70,80] (70,80] (60,70] (70,80] (30,40] ## [610] (50,60] (30,40] (40,50] (40,50] (50,60] (90,100] (30,40] ## [617] (90,100] (50,60] (80,90] (50,60] (60,70] (40,50] (50,60] ## [624] (50,60] (50,60] (50,60] (50,60] (80,90] (60,70] (40,50] ## [631] (70,80] (60,70] (80,90] (60,70] (40,50] (70,80] (0,10] ## [638] (40,50] (50,60] (60,70] (60,70] (80,90] (40,50] (10,20] ## [645] (70,80] (60,70] (80,90] (70,80] (80,90] (60,70] (40,50] ## [652] (30,40] (70,80] (50,60] (0,10] (40,50] (40,50] (50,60] ## [659] (50,60] (20,30] (50,60] (70,80] (60,70] (60,70] (80,90] ## [666] (40,50] (50,60] (50,60] (90,100] (60,70] (80,90] (80,90] ## [673] (80,90] (70,80] (100,110] (90,100] (40,50] (60,70] (30,40] ## [680] (50,60] (50,60] (10,20] (70,80] (80,90] (30,40] (60,70] ## [687] (60,70] (70,80] (50,60] (50,60] (50,60] (30,40] (30,40] ## [694] (40,50] (60,70] (70,80] (50,60] (70,80] (80,90] (70,80] ## [701] (70,80] (70,80] (40,50] (80,90] (50,60] (50,60] (40,50] ## [708] (0,10] (60,70] (30,40] (30,40] (50,60] (70,80] (40,50] ## [715] (40,50] (70,80] (30,40] (120,130] ## 13 Levels: (0,10] (10,20] (20,30] (30,40] (40,50] (50,60] (60,70] ... (120,130] The observations have now been grouped to the classes. You can see this explicitly in the data viewer: View(air_quality_data) Important Notes: What have we done here? The first value under the NO2_est column is 61, this value falls between 61-70 and hence under the Group column is was classed into the (61,70] interval by the cut() function. The second value in NO2_est is 59, and hence it was classed into the (51,60] interval, and so on. Computing the Frequency Distribution table We can now generate our frequency table and hence determine frequency and cumulative frequency of the ambient levels of NO\\(_2\\) in Eixample. We perform by using the table() function to tabulate the frequency of values that were grouped within an interval using the Group column. table(air_quality_data$Groups) ## ## (0,10] (10,20] (20,30] (30,40] (40,50] (50,60] (60,70] (70,80] ## 6 17 24 68 131 137 109 116 ## (80,90] (90,100] (100,110] (110,120] (120,130] ## 72 20 7 7 4 Using table() function only shows results in the Console - lets store the table results in a data frame object and call it frequency_results: frequency_results &lt;- data.frame(table(air_quality_data$Groups)) frequency_results ## Var1 Freq ## 1 (0,10] 6 ## 2 (10,20] 17 ## 3 (20,30] 24 ## 4 (30,40] 68 ## 5 (40,50] 131 ## 6 (50,60] 137 ## 7 (60,70] 109 ## 8 (70,80] 116 ## 9 (80,90] 72 ## 10 (90,100] 20 ## 11 (100,110] 7 ## 12 (110,120] 7 ## 13 (120,130] 4 You can see column names Var1 and Freq. The Var1 is the original Groups columns which incidentally been renamed to Var1. The Freq column was generated from the table() function. We can rename the 1st and 2nd columns using colnames(). # rename first column t9 &quot;Groups&quot; # rename second column to &quot;Frequency&quot; # print new variable names in console using names() function colnames(frequency_results)[1] &lt;- &quot;Groups&quot; colnames(frequency_results)[2] &lt;- &quot;Frequency&quot; names(frequency_results) ## [1] &quot;Groups&quot; &quot;Frequency&quot; frequency_results ## Groups Frequency ## 1 (0,10] 6 ## 2 (10,20] 17 ## 3 (20,30] 24 ## 4 (30,40] 68 ## 5 (40,50] 131 ## 6 (50,60] 137 ## 7 (60,70] 109 ## 8 (70,80] 116 ## 9 (80,90] 72 ## 10 (90,100] 20 ## 11 (100,110] 7 ## 12 (110,120] 7 ## 13 (120,130] 4 Finally, we derive the relative frequency i.e., a percentage that is derived by dividing each frequency value from a group by the total number of observations (i.e., in this case: 718). We can add the relativeFreq column to the frequency_results table. # generate a new column frequency_results$relativeFreq &lt;- frequency_results$Frequency/718 Interpretation of frequency: The above table output show the frequency distribution of a set of concentrations for Nitrogen Dioxide measured in Eixample (in Barcelona). The group with the highest frequency value is 50-60ppb (i.e., 137) which accounts for 0.1908 (19.08%) of the data. These measurements typically fall under the category that’s considered to cause moderate harm to humans. Let’s add the cumulative frequency and cumulative relative frequency i.e., percentage using this code below: # add cumulativeFreq column to the data frame by adding Frequency using cumsum() function frequency_results$cumulativeFreq &lt;- cumsum(frequency_results$Frequency) # add cumulativeRelFreq column to the data frame by adding Frequency using cumsum() function frequency_results$cumulativeRelFreq &lt;- cumsum(frequency_results$relativeFreq) # print table results frequency_results ## Groups Frequency relativeFreq cumulativeFreq cumulativeRelFreq ## 1 (0,10] 6 0.008356546 6 0.008356546 ## 2 (10,20] 17 0.023676880 23 0.032033426 ## 3 (20,30] 24 0.033426184 47 0.065459610 ## 4 (30,40] 68 0.094707521 115 0.160167131 ## 5 (40,50] 131 0.182451253 246 0.342618384 ## 6 (50,60] 137 0.190807799 383 0.533426184 ## 7 (60,70] 109 0.151810585 492 0.685236769 ## 8 (70,80] 116 0.161559889 608 0.846796657 ## 9 (80,90] 72 0.100278552 680 0.947075209 ## 10 (90,100] 20 0.027855153 700 0.974930362 ## 11 (100,110] 7 0.009749304 707 0.984679666 ## 12 (110,120] 7 0.009749304 714 0.994428969 ## 13 (120,130] 4 0.005571031 718 1.000000000 Interpretation of cumulative frequency: The above table output show the cumulative frequency distribution ambient concentrations for Nitrogen Dioxide measured in Eixample (in Barcelona). We can see that there are 246 measurements or less with N0\\(_2\\) concentrations to be considered as negligible or low impact to health (&lt;50ppb). This corresponds to 0.3426 (34.26%) of the data. Conversely, we can also say - we can see that there are 472 measurements with N0\\(_2\\) concentrations more than 50ppb which is considered to be moderate or high impact to human health. This corresponds to 0.6573 (65.73%) of the data. Graphical representation of frequency data The frequency table for frequencies and cumulative frequencies can be graphical represented in a form of histogram and frequency diagram respectively. Now, we need the data must be in its original form (i.e., not grouped) to plot the histogram, and we will need to use the classes object which we created earlier on from the seq() function so as to use as breaks in the hist() plot function: hist(air_quality_data$NO2_est, breaks = classes) The above graph is not publication-worthy. It is missing key details such as the title and label for the x-axis. Let’s apply some cosmetics such as a main title and label for the x-axis hist(air_quality_data$NO2_est, breaks = classes, main = &quot;Histogram for NO2 in Barcelona&quot;, xlab = &quot;NO2 estimates (ppb)&quot;) Interpretation of histogram: The above figure output describes the shape for ambient measures of NO\\(_2\\) in Barcelona which appears bell-shaped centered around 60ppb. Note that the frequency bars in this graph are essentially the same as the frequency values in the table. Lastly, we then compute its cumulative frequency with cumsum() to support the interpretation. The coding needs a bit of hacking because we need to force a starting zero element for this graph to work. cumfreq0 &lt;- c(0, cumsum(frequency_results$Frequency)) plot(classes, cumfreq0, main=&quot;Cumulative Frequency for N02 in Barcelona&quot;, xlab=&quot;NO2 estimates (ppb)&quot;, ylab=&quot;Cumulative Frequencies&quot;) lines(classes, cumfreq0) Descriptive and central tendency measures We have used frequency distribution to describe the distribution about the data for air pollution in Barcelona. The description is at face-value though. Central tendency measures contains a list of summary measurements that allows the user to summarize the data to some central measure and to gauge the spread of the data (i.e., errors/departures from central measure). It is best for continuous variables, we can hence compute the following summary measurements (watch video below see to definitions): Mean Median Variance and Standard deviation Minimum and Maximum values Upper and Lower Quartiles (or Interquartile ranges) [Theory]: Central tendency measures (Length: 17:12 minutes) Watch on YouTube LINK [Theory]: Range values and interquartile ranges (Length: 18:32 minutes) Watch on YouTube [LINK] To compute the summary statistics rapidly – simply use the summary() function on the variable of interest (i.e., NO2_est from the original dataset). # compute all descriptive summaries measurements summary(air_quality_data$NO2_est) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 46.00 59.00 59.69 74.00 130.00 As you can see using summary() automatically gives you almost all the summary estimates needed for interpretation. The NO\\(_2\\) air levels in Barcelona was 59.69ppb (with median 59.00pbb). The lowest and highest values are 2.0ppb and 130.0ppb, respectively (with 25th and 75th percentiles being 46.00ppb and 74.00pbb, respectively.) Finally, you can compute the standard deviation using the sd() function as follows for income: # compute all descriptive summaries measurements sd(air_quality_data$NO2_est) ## [1] 20.60876 [Theory]: Variance &amp; standard deviation (Length: 13:39 minutes) Watch on YouTube [LINK] The standard deviation is ± 20.61pbb – this is the error around the mean of NO\\(_2\\) i.e., estimated as 59.69ppb. To visualise the distribution of a continuous variable – your ‘go-to’ plot is a boxplot. You can use the function boxplot() to generate one. Type the following to churn it: # Box plot boxplot(air_quality_data$NO2_est, ylab = &quot;NO2 estimates (ppb)&quot;, main=&quot;Box plot: Summary of Nitrogen Dioxide in Barcelona&quot;) The above box plot is essential the visual representation of the summary results churned by the summary(). Here is the concise interpretation of the above results: Interpretation: The overall mean air pollution levels for NO\\(_2\\) in Eixample (Barcelona) from 718 observation was 59.69ppb (with one SD of ± 20.63ppb). 25% (at the lower end i.e., lower quartile) of the distribution for air population levels for Nitrogen Dioxide are below 46.00ppb (which is considered to cause low health impact); while from 75% onwards (i.e., upper quartile) of the distribution has air pollution levels of N0\\(_2\\) which is above 74.00pbb (which is consider to considered to cause some moderate health impact). The overall range in the distribution is 128ppb where the lowest observed value is 2.0ppb (minimum) and the highest observed value is 130ppb (maximum) "],["part-4-introduction-to-probability-distribution.html", "Part 4: Introduction to Probability Distribution Quick Recap What are Probability Distributions? How to generate a Normal Distribution curve Using the Normal Distribution for making a prediction Finished!", " Part 4: Introduction to Probability Distribution Quick Recap Last week’s tutorials, in [part three of the workbook], we learnt how to perform some basic descriptive analysis of air pollution data (specifically 718 observations of Nitrogen Dioxide (NO\\(_2\\)) in [Eixample, Barcelona]. We derived the following outputs: Frequency distribution and table for ambient NO\\(_2\\) levels in Barcelona, which told us how often a particular measure for NO\\(_2\\) was observed. For instance, we found that in Barcelona, the common measure for ambient NO\\(_2\\) were values that fell in the range of 50-60ppb that accounted for 19.08% of the data. Cumulative frequency distribution of ambient NO\\(_2\\) levels in Barcelona, which - based on a stated value/threshold of interest - indicated how often a particular measure for N0\\(_2\\) was observed. For example, based on this threshold &lt;50ppb, we found that it corresponds to 246 measurements for NO\\(_2\\) below 50ppb which composed of 0.3426 (34.26%) of the data. Histogram, which was simply a graphically representation that reflected our Frequency distribution. For instance, the image we produced last week: Central Tendency Measures, Variance &amp; Standard Deviation: These are a set of statistical estimates we used to provide a concise summary of the data. There more important measures were the mean (or average) and standard deviation. Recall in our air pollution data, we found the overall mean air pollutions level for NO\\(_2\\) in [Eixample, Barcelona] was 59.69ppb with a standard deviation (i.e., error) of ±20.63pbb. These four recap points were brought up because they are all connected to Probability Distributions. Probability distributions are indeed the basis of data sciences, we actually use to these help model our world, enabling us to estimate or predict the probability that an event (e.g., disease outbreak, mosquito infestation, crime outcome, wildfire hazard etc.,) may occur or not. More importantly, we use these specifically in Hypothesis Testing when conducting Inferential Statistics - of course, this important point is a story for another day as some of you will see its connection in term two’s Geography in the Field II (GEOG0014) and/or in Introduction to Quantitative Research Methods (POLS0008). In this tutorial, you will be introduced to the Normal Distribution, a famous one which has its place in statistical analysis. [Alright, we’re in final stretch - let’s do this!] What are Probability Distributions? [Context]: What are Probability Distributions? (Length: 04:34 minutes) Watch on [YouTube] Do remember a time in our GSCEs when we were asked to compute the probability of an event such as - let’s say - what the probability of throwing a die just once and it showing the value “six” as an outcome? The obvious answer to that is 1/6. Here, we computed the probability of just a single outcome. Probability Distributions on the other hand allows us to compute probabilities for ALL possible outcomes. There are [SEVERAL] different types of probability distributions; however, the most common types you will come across in data science are: Normal Distribution for continuous data that are measurable (e.g., height, age, air pollution levels etc.) Poisson Distribution for discrete data that are countable (e.g., number of COVID-19 cases, reported number of phone snatched victims etc.,) Uniform Distribution for continuous or discrete data are measurable or countable, respectively, where every data point is assumed to have an equal chance of being observed. Bernoulli Distribution for binary data that contain two categories that are of a Yes/No, Success/Failure, In/Out or Win/Lose format (e.g., a disease outcome of a patient (Yes, No), A victim of theft (Yes, No) and so on). Probability distributions are always represented in a graphical format as a curve. As stated, the Normal Distribution, which is a bell-shaped curve, is the most “famous” and widely used probability distribution because - surprisingly - a lot of measurable phenomena that are continuous data tend to be approximately bell-shaped. For instance, let’s take our previous example of the histogram graph we created from the NO\\(_2\\) data in Barcelona. While the graph shows the frequency of NO\\(_2\\) data using rectangles. The height of a rectangle (the vertical axis) basically represents the distribution frequency for the NO\\(_2\\) measure, as in, the amount, or how often that value was measured. Now, notice how the above figure output takes on shape that is somewhat akin to that of a bell-curve? The histogram indeed describes the shape for the ambient NO\\(_2\\) measures as a distribution that’s bell-shaped which was centered on the mean value of 59.69ppb and error (i.e., standard deviation away from the mean) was plus or minus 20.63pbb. This output reproduced in image [A] was generated from the data that follows Normal Distribution. Sometimes, we can be in a situation where we are dealing with a data science problem but do not have any data at our disposal. But with probability distributions, we can do a work around if have prior knowledge or prior information of the problem. For instance, it is possible for us to generate a theoretical distribution for these NO\\(_2\\) values based on our knowledge of its mean and standard deviation with the assumption that its bell-shaped in the first place as shown in image [B]. This curve will show us the probability of observing every single possible value for the NO\\(_2\\) in Barcelona, as well as simulate a similar dataset. Let us learn how to generate this curve shown in image B as a step-by-step demonstration. How to generate a Normal Distribution curve [Coding]: How to create the Normal Distribution curve (Length: 32:53 minutes) Watch on [YouTube] We will need to re-import the Barcelona_Air_Pollution_data.csv into RStudio and call this object air_quality_data. Recall, we imported this data in last week’s workshop - you can see the instructions [HERE]). air_quality_data &lt;- read.csv(&quot;Barcelona_Air_Pollution_data.csv&quot;) IMPORTANT NOTE: Importing data and setting up your working directory has been covered extensively in last week’s content. In case you are having trouble, please refer to the learning materials [HERE] Let’s use the mean(), sd(), min() and max() function on the NO2_est variable to compute some important values which will use as a basis to create the normal distribution plot. # calculates the mean of NO2 mean(air_quality_data$NO2_est) ## [1] 59.6922 # calculates the standard deviation of NO2 sd(air_quality_data$NO2_est) ## [1] 20.60876 # finds the lowest value for NO2 min(air_quality_data$NO2_est) ## [1] 2 # finds the maximum value for NO2 max(air_quality_data$NO2_est) ## [1] 130 For the graph, we will need to create a template for the x-axis in a way that contains clean intervals of 10’s starting from 0, and ending up to 130 so as to capture the minimum and maximum values for NO\\(_2\\). # create the intervals for the x-axis intervals &lt;- seq(0, 130, 10) intervals ## [1] 0 10 20 30 40 50 60 70 80 90 100 110 120 130 Now, we are going to create the a vector for the x-axis which will contain a sequence of EVERY possible value for the NO\\(_2\\) within the range of 2ppb (minimum) to 130ppb (maximum). Here, we will create a series of clean values of equal intervals of 0.01 starting from 2 (lowest possible value), and up to 130 (highest possible value) that we know from our dataset. # create actual x-axis for every single possible value between 2 to 130 x_axis &lt;- seq(2, 130, by = 0.01) IMPORTANT NOTE: The above code will create a sequence of values starting from 2 and ending at 130. These values will be 2.00, 2.01, 2.02, 2.03 … and so on and so forth. Now, we are going to use a new function called dnorm() which allows us to compute the probability density for each of the values along the x_axis object we created. In this function, we will need to specify our prior information which is our knowledge about the pollution levels in Barcelona. Hence, we will specify the mean and standard deviation. # we use to x_axis and our known mean and SD probability_estimates &lt;- dnorm(x_axis, mean = 59.69, sd = 20.63) IMPORTANT NOTE: The above code is estimating the exact probability of each value contained in that x_axis object. Hence, each value i.e., 2.00, 2.01, 2.02, 2.03 … and so on and so forth, will have its own probability estimate which is called a probability density Now that we have the key ingredients to visualise our probability distribution for the NO\\(_2\\) values in Barcelona. We can proceed to create our graph from scratch. We start with using the plot() and applying the labels. # create a plot from scratch plot(x_axis, probability_estimates, type = &quot;l&quot;, lwd = 3, axes = FALSE, main = &quot;Probability [Normal] Distribution for NO2 in Barcelona&quot;, xlab = &quot;NO2 values&quot;, ylab = &quot;Probability Density&quot;) IMPORTANT NOTE: This is full-on custom graph we are making in RStudio. As there is no conventional way of producing such a graph. This is where the programming and hacks come to play, and brute forcing it [John Wick style] until we get the desired result! About the code: plot(x_axis, probability_estimates...), we are saying apply the values from x_axis on to the x-axis and those from probability_estimates on to the y-axis. The type = \"l\", we are saying that this is a line/curve plot. lwd = 3, this controls the thickness of the line width. Here, we have set this to 3. axes = FALSE, we have deliberately set this to not appear. We want to fully customise our graph as you will see in the next set of codes. main = \"...\", set the title of plot xlab = \"...\", set the title for the x-axis ylab = \"...\", set the title for the y-axis Let’s add our custom made x-axis. # create a plot from scratch plot(x_axis, probability_estimates, type = &quot;l&quot;, lwd = 3, axes = FALSE, main = &quot;Probability [Normal] Distribution for NO2 in Barcelona&quot;, xlab = &quot;NO2 values&quot;, ylab = &quot;Probability Density&quot;) # add x-axis and use the interval breaks we created earlier on axis(1, at = intervals) Let see the maximum probability density estimate, this information will help us to customise the y-axis appropriately max(probability_estimates) ## [1] 0.01933797 Its 0.01933797. Okay then, so let’s add y-axis which starts from 0 and end at 0.020 spaced at intervals of 0.005. # create a plot from scratch plot(x_axis, probability_estimates, type = &quot;l&quot;, lwd = 3, axes = FALSE, main = &quot;Probability [Normal] Distribution for NO2 in Barcelona&quot;, xlab = &quot;NO2 values&quot;, ylab = &quot;Probability Density&quot;) # add x-axis and use the interval breaks we created earlier on axis(1, at = intervals) # add y-axis and start from 0 and end at 0.020 at intervals of 0.005 axis(2, at = c(0, 0.005, 0.010, 0.015, 0.020)) We created the normal distribution curve. Now, we add the reference lines for the mean as well as the lower and upper bounds for the standard deviation. # create a plot from scratch plot(x_axis, probability_estimates, type = &quot;l&quot;, lwd = 3, axes = FALSE, main = &quot;Probability [Normal] Distribution for NO2 in Barcelona&quot;, xlab = &quot;NO2 values&quot;, ylab = &quot;Probability Density&quot;) # add x-axis and use the classes breaks from the histogram axis(1, at = intervals) # add y-axis and start from 0 and end at 0.020 at intervals of 0.005 axis(2, at = c(0, 0.005, 0.010, 0.015, 0.020)) # add vertical line for mean = 59.69 abline(v=59.69, lwd = 1.5, col=&quot;black&quot;) # add two vertical lines for the plus and minus values for our sd = 20.63 # lower = 59.69 - 20.63 # upper = 59.69 + 20.63 abline(v=c(39.16, 80.22), lty=&quot;dashed&quot;, lwd = 1.5, col=&quot;red&quot;) IMPORTANT NOTE: The centre black line represents the position of the mean on the x-axis. Whilst the red dashed lines are the plus and minus limits from the standard deviations. About the code: abline(...), we are saying apply the reference line on main plot. v = ..., we are adding a vertical line to a specified value on the x-axis lty = \"dashed\", controls the style format of line. Here, its “dashed” This is the expected output. Let’s demonstrate how to use it for making a prediction about the air pollution levels for NO\\(_2\\). Using the Normal Distribution for making a prediction [Coding]: Probabilistic predictions (Length: 17:50 minutes) Watch on [YouTube] From the above graph - the image shows that for each value of NO\\(_2\\) along the x-axis corresponds to some probability density estimate on the y-axis. For instance, the probability density for NO\\(_2\\) levels being exactly equal to 63ppb is 0.01911 (1.911%); and for an example, the probability density for NO\\(_2\\) levels being exactly equal to 80ppb is 0.01191 (1.1191%) etc. Now, traditionally, we are not interested in computing these probability densities for these exact values because these estimates for a single value are always negligible! But rather - what we are interested in estimating the cumulative probability density i.e., finding the probability that such an outcome is either “up to” (or at most) or alternatively “at least” some threshold, or even being “within a range” of values. The questions we usually ask to predict something are typically framed like: What is the probability that the air pollution levels of NO\\(_2\\) in Barcelona reaches up to (or is at most) 45ppb? What is the probability that the air pollution levels of NO\\(_2\\) in Barcelona exceeds (or is at least) 80ppb? What is the probability that the air pollution levels of NO\\(_2\\) in Barcelona falls within the range of (or is between) 45-80ppb? Let see how to answer each scenario. Scenario 1: The Probabilities for (“at most”) Suppose that we want to compute the probability that air pollution levels for NO\\(_2\\) in Barcelona reaches up to 45ppb. First of all, note that this is an “at most” scenario. The way in which this is represented graphically under our Normal Distribution where our mean is 59.69 and a standard deviation of ±20.63 is: The red shaded area on the left-side under this normal curve represents the cumulative probabilities for the NO\\(_2\\) levels being at most 45ppb. Here, we can calculate this cumulative probability by using the pnorm() function. All we need to do is insert our value of interest along with the mean and standard deviation to obtain our result. # compute the cumulative probability for 45 pnorm(45, mean=59.69, sd=20.63) ## [1] 0.2382108 We get 0.2397141 or 23.97%. So we have therefore predicted that there’s a 23.97% chance that NO\\(_2\\) levels in Barcelona reach up to 45ppb. Please note that the code for generating the above plot for this scenario is shown below. It builds on that custom plot code we made earlier: # create a plot from with shaded areas plot(x_axis, probability_estimates, type = &quot;l&quot;, lwd = 3, axes = FALSE, main = &quot;Probability of NO2 being at most (or reaching) 45ppb&quot;, xlab = &quot;NO2 values&quot;, ylab = &quot;Probability Density&quot;) # add x-axis and use the classes breaks from the histogram axis(1, at = intervals) # add y-axis and start from 0 and end at 0.020 at intervals of 0.005 axis(2, at = c(0, 0.005, 0.010, 0.015, 0.020)) # add mean reference line abline(v=59.69, lwd = 1.5, lty = &quot;dashed&quot;, col=&quot;black&quot;) # add shaded area under the curve (left-side) polygon( c(min(x_axis), x_axis[x_axis &lt; 45], 45), c(0, probability_estimates[x_axis &lt; 45], 0), col=&quot;red&quot;) IMPORTANT NOTE: In the above custom code for creating the plot, we have just added the polgyon() function to shade that portion of interest in red. You can replace the 45 with a different value if you want to reproduce this “left-sided” plot. Scenario 2: The Probabilities for (“at least”) Suppose that we want to compute the probability that air pollution levels for NO\\(_2\\) in Barcelona exceeds 80ppb. Please, note that this problem is an “at least” scenario. The way in which this is represented graphically under our Normal Distribution where we use our mean of 59.69 and standard deviation of ±20.63 is as follows: The red shaded area on the right-side under this normal curve represents the cumulative probabilities for the NO\\(_2\\) levels being at least 80ppb. Note that when calculating probabilities of this scenario, we need to do 1 - pnorm() to calculate the red shaded area on the right side of this graph. # compute the cumulative probability for 80 1 - pnorm(80, mean=59.69, sd=20.63) ## [1] 0.1624377 We get 0.1612494 or 16.12494%. So we have therefore predicted that there’s a 16.12% chance that air concentrations of NO\\(_2\\) levels in Barcelona exceeds 80ppb. Again, note that the code for generating the above plot for this scenario is shown below. It builds on that custom plot code we made earlier: # create a plot from scratch plot(x_axis, probability_estimates, type = &quot;l&quot;, lwd = 3, axes = FALSE, main = &quot;Probability of NO2 being at least (or exceeding) 80ppb&quot;, xlab = &quot;NO2 values&quot;, ylab = &quot;Probability Density&quot;) # add x-axis and use the classes breaks from the histogram axis(1, at = intervals) # add y-axis and start from 0 and end at 0.020 at intervals of 0.005 axis(2, at = c(0, 0.005, 0.010, 0.015, 0.020)) # add mean reference line abline(v=59.69, lwd = 1.5, lty = &quot;dashed&quot;, col=&quot;black&quot;) # add shaded area under the curve (right-side) polygon( c(x_axis[x_axis&gt;=80], 80), c(probability_estimates[x_axis&gt;=80], probability_estimates[x_axis==max(x_axis)]), col=&quot;red&quot;) IMPORTANT NOTE: In the above custom code for the plot, we have only added the polgyon() function to shade that portion of interest in red. You can replace the 80 with a different value if you want to reproduce this “right-sided” plot. Scenario 3: The Probabilities for (“within a range”) Suppose that we want to compute the probability that air pollution levels for NO\\(_2\\) in Barcelona falls between 45ppb and 80ppb. Please, note that this problem is an “within a range” scenario. The way in which this problem is represented graphically under our Normal Distribution with mean of 59.69 and standard deviation of ±20.63 is: The red shaded area in the middle portion under this normal curve represents the cumulative probabilities for the NO\\(_2\\) levels being between 45ppb and 80ppb. Note that when calculating probabilities of this scenario, we only need to do pnorm(80...) - pnorm(45..) to calculate the red shaded area in middle portion of this graph. # compute the cumulative probability for 80 pnorm(80, mean=59.69, sd=20.63) - pnorm(45, mean=59.69, sd=20.63) ## [1] 0.5993516 We get 0.5993516 or 59.93516%. So we have therefore predicted that there’s a 59.93% chance that air concentrations of NO\\(_2\\) levels in Barcelona fall between 45ppb and 80ppb. Again, note that the code for generating the above plot for this scenario is shown below. It builds on that custom plot code we made earlier: # create a plot from scratch plot(x_axis, probability_estimates, type = &quot;l&quot;, lwd = 3, axes = FALSE, main = &quot;Probability of NO2 being between 45ppb and 80ppb&quot;, xlab = &quot;NO2 values&quot;, ylab = &quot;Probability Density&quot;) # add x-axis and use the classes breaks from the histogram axis(1, at = intervals) # add y-axis and start from 0 and end at 0.020 at intervals of 0.005 axis(2, at = c(0, 0.005, 0.010, 0.015, 0.020)) # add shaded area under the curve (middle) polygon( c(45, x_axis[x_axis&gt;=45 &amp; x_axis&lt;=80], 80), c(0, probability_estimates[x_axis&gt;=45 &amp; x_axis&lt;=80] ,0), col=&quot;red&quot;) # add mean reference line abline(v=59.69, lwd = 1.5, lty = &quot;dashed&quot;, col=&quot;black&quot;) IMPORTANT NOTE: In the above custom code for the plot, we have only added the polgyon() function to shade that portion of interest in red. You can replace the lower (i.e., 45) and upper (i.e., 80) values with a different numbers to shade out the middle portion under this curve plot. Finished! And that’s all to it. That’s how we conduct probabilistic modelling and this example is simply scratching the surface. Probability theory is a fascinating area which is the heart and soul of Inferential statistics particularly for Hypothesis testing, but it also opens the doors for more advanced fields such as Machine Learning and Bayesian statistics. Anyways, well done for completing these workshops. We know, they were long… and a grueling process. As an appreciation, we end with compliments by expression though this iconic scene from one of our all time great anime, Neon Genesis Evangelion. "],["formative-assignment.html", "Formative Assignment", " Formative Assignment Please read this section carefully: The Formative Sheet contains all the questions for this independent work. Please try to complete the tasks before the week commencing on Monday 4th December 2023. You will need this for your Thinking Geographically group tutorials in that week. It will be an opportunity for have both a student-led peer-to-peer review of the result output. Click [HERE] to download the Formative Worksheet. Use the Worksheet Template to include all results and outputs. To streamline things for you – there no need to go over 2-sides of A4. You can include everything here (i.e., interpretation, quantitative results, and figure outputs). The font size has been defaulted to 11. Please bring this sheet your Thinking Geographically group tutorials in the week commencing on Monday 4th December 2023. Please use this dataset to complete the coursework. Click [HERE] to download the required dataset. "],["appendendum.html", "Appendendum Appendix 1: List of functions used in this workshop Appendix 2: List of symbols used in this workshop", " Appendendum Appendix 1: List of functions used in this workshop Function Description of use setwd() Use it to set the work directory to folder with stored data read.csv() Use it to import and open a .csv excel spreadsheet write.csv() Use it to export and save data as a .csv excel spreadsheet data.frame() Use it to create data frame object View() Use it to examine your data set in a data viewer merge() Use it to merge two data frames together c() Use it to create a list of data items to create a vector object exp() A simple mathematical function - exponential log() A simple mathematical function - logarithmic head() Use on data frame to see the first couple of row observations tail() Use on data frame to see the last couple of row observations min() Use on variable to see the lowest value max() Use on variable to see the highest value seq() Use it to generate a sequence of numbers cut() Use it to class or categorize a continuous variable table() Create a table, or to perform cross-tabulation cumsum() Compute the cumulative sums of a variable hist() Plot a histogram plot() Make a general plot axis() Supplementary code making custom plots for axis. Follow-up to plot() polygon() Supplementary code making custom plots for areas under curves. Follow-up to plot() lines() Used as a follow-up to plot()to add a line/curve to a plot abline() Used as a follow-up to plot()to add vertical/horizontal lines to a plot colnames() To rename columns summmary() Reports the min, max, median, mean, and IQRs sd() Reports the standard deviation boxplot() Plot a box plot dnorm() Generates a normal distribution from mean &amp; standard deviation pnorm() Predicts probability under a normal distribution Appendix 2: List of symbols used in this workshop Symbol Description of use $ Dollar sign for accessing a variable within a data frame &lt;- Operator sign for assigning values to an object + Addition - Subtraction * Multiplication / Divisor ^ Raise to power "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
